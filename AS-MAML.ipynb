{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6a69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tristandeleu/pytorch-maml\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "from maml.datasets import get_benchmark_by_name\n",
    "from maml.metalearners.maml_sharp import ModelAgnosticMetaLearning\n",
    "\n",
    "from sam import SAM\n",
    "from sam_folder.model.smooth_cross_entropy import smooth_crossentropy\n",
    "from sam_folder.utility.bypass_bn import enable_running_stats, disable_running_stats\n",
    "from sam_folder.model.wide_res_net import WideResNet\n",
    "from sam_folder.utility.step_lr import StepLR\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665cc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_data():\n",
    "    def __init__(self):\n",
    "        self.folder = '/data/'\n",
    "        self.dataset0 = \"omniglot\"\n",
    "        self.dataset1 = \"miniimagenet\"\n",
    "        self.dataset2 = \"doublemnist\"\n",
    "        self.dataset3 = \"triplemnist\"\n",
    "        self.output_folder = '\\data\\results'\n",
    "        self.num_ways = 20\n",
    "        self.num_shots = 1\n",
    "        self.num_shots_test = 15\n",
    "        self.hidden_size = 64\n",
    "        self.batch_size = 4\n",
    "        self.num_steps = 5\n",
    "        self.num_epochs = 600\n",
    "        self.num_batches = 100\n",
    "        self.step_size = 0.001\n",
    "        self.first_order = True\n",
    "        self.meta_lr = 0.001\n",
    "        self.num_workers = 0\n",
    "        self.verbose = True\n",
    "        self.use_cuda = True\n",
    "        \n",
    "        self.alpha = 0.05\n",
    "        self.adap = True\n",
    "        self.SAM_lower = True\n",
    "        self.m = 1\n",
    "        self.delta = 0\n",
    "        \n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "        self.isMomentum = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18878287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Creating folder `/data/Pestian_Lab/Projects/Mental_Health/Mental_Health_Trajectories/anjm1m/Sharp-MAML-mod/lower/data/results/2024-02-08_124044`\n",
      "INFO:root:Saving configuration file in `/data/Pestian_Lab/Projects/Mental_Health/Mental_Health_Trajectories/anjm1m/Sharp-MAML-mod/lower/data/results/2024-02-08_124044/config.json`\n"
     ]
    }
   ],
   "source": [
    "args = input_data()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)\n",
    "device = torch.device('cuda' if args.use_cuda\n",
    "                      and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if (args.output_folder is not None):\n",
    "    if not os.path.exists(args.output_folder):\n",
    "        os.makedirs(args.output_folder)\n",
    "        logging.debug('Creating folder `{0}`'.format(args.output_folder))\n",
    "        \n",
    "folder = os.path.join(args.output_folder, time.strftime('%Y-%m-%d_%H%M%S'))\n",
    "\n",
    "os.makedirs(folder)\n",
    "logging.debug('Creating folder `{0}`'.format(folder))\n",
    "\n",
    "args.folder = os.path.abspath(args.folder)\n",
    "args.model_path = os.path.abspath(os.path.join(folder, 'model.th'))\n",
    "# Save the configuration in a config.json file\n",
    "with open(os.path.join(folder, 'config.json'), 'w') as f:\n",
    "    json.dump(vars(args), f, indent=2)\n",
    "logging.info('Saving configuration file in `{0}`'.format(\n",
    "              os.path.abspath(os.path.join(folder, 'config.json'))))\n",
    "\n",
    "# Select dataset from: dataset0 for miniimagenet, dataset1 for omniglot, dataset2 for doubleMNIST, & dataset3 for tripleMNIST\n",
    "benchmark = get_benchmark_by_name(args.dataset3,\n",
    "                                     args.folder,\n",
    "                                     args.num_ways,\n",
    "                                     args.num_shots,\n",
    "                                     args.num_shots_test,\n",
    "                                     hidden_size=args.hidden_size)\n",
    "\n",
    "meta_train_dataloader = BatchMetaDataLoader(benchmark.meta_train_dataset,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=args.num_workers,\n",
    "                                               pin_memory=True)\n",
    "meta_val_dataloader = BatchMetaDataLoader(benchmark.meta_val_dataset,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=args.num_workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "# meta_optimizer = torch.optim.Adam(benchmark.model.parameters(), lr=args.meta_lr)\n",
    "base_optimizer = torch.optim.Adam\n",
    "meta_optimizer = SAM(benchmark.model.parameters(), base_optimizer, rho=args.alpha,\n",
    "                     adaptive=args.adap, lr=args.meta_lr)\n",
    "\n",
    "metalearner = ModelAgnosticMetaLearning(benchmark.model,\n",
    "                                        meta_optimizer,\n",
    "                                        adap=args.adap,\n",
    "                                        alpha=args.alpha,\n",
    "                                        delta=args.delta,\n",
    "                                        SAM_lower=args.SAM_lower,\n",
    "                                        first_order=args.first_order,\n",
    "                                        num_adaptation_steps=args.num_steps,\n",
    "                                        step_size=args.step_size,\n",
    "                                        m=args.m,\n",
    "                                        beta1=args.beta1,\n",
    "                                        beta2=args.beta2,\n",
    "                                        isMomentum=args.isMomentum,\n",
    "                                        beta=args.meta_lr,\n",
    "                                        loss_function=benchmark.loss_function,\n",
    "                                        device=device)\n",
    "\n",
    "\n",
    "best_value = None\n",
    "\n",
    "logging.getLogger('PIL').setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46a562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/data/Pestian_Lab/Projects/Mental_Health/Mental_Health_Trajectories/anjm1m/maml-momentum-final/maml/metalearners/maml_sharp.py:263\u001b[0m, in \u001b[0;36mModelAgnosticMetaLearning.train\u001b[0;34m(self, dataloader, max_batches, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mmax_batches, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m results \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iter(dataloader, max_batches\u001b[38;5;241m=\u001b[39mmax_batches):\n\u001b[1;32m    264\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    265\u001b[0m             postfix \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_outer_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n",
      "File \u001b[0;32m/data/Pestian_Lab/Projects/Mental_Health/Mental_Health_Trajectories/anjm1m/maml-momentum-final/maml/metalearners/maml_sharp.py:281\u001b[0m, in \u001b[0;36mModelAgnosticMetaLearning.train_iter\u001b[0;34m(self, dataloader, max_batches)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_batches \u001b[38;5;241m<\u001b[39m max_batches:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m num_batches \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_batches:\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:27\u001b[0m, in \u001b[0;36mBatchMetaCollate.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_task(task) \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:21\u001b[0m, in \u001b[0;36mBatchMetaCollate.collate_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([task[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(task))])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, OrderedDict):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict([(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_task(subtask))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (key, subtask) \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([task[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(task))])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, OrderedDict):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict([(key, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubtask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (key, subtask) \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:19\u001b[0m, in \u001b[0;36mBatchMetaCollate.collate_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_task\u001b[39m(\u001b[38;5;28mself\u001b[39m, task):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TorchDataset):\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([task[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(task))])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, OrderedDict):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict([(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_task(subtask))\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (key, subtask) \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mitems()])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/dataloader.py:19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_task\u001b[39m(\u001b[38;5;28mself\u001b[39m, task):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, TorchDataset):\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(task))])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(task, OrderedDict):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict([(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_task(subtask))\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (key, subtask) \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mitems()])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/task.py:63\u001b[0m, in \u001b[0;36mSubsetTask.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSubset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/torch/utils/data/dataset.py:311\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/utils/data/task.py:50\u001b[0m, in \u001b[0;36mConcatTask.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/torch/utils/data/dataset.py:257\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchmeta/datasets/triplemnist.py:244\u001b[0m, in \u001b[0;36mTripleMNISTDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 244\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    245\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/mhtrajectories/usman/anaconda/envs/sparknlptorch/lib/python3.8/site-packages/h5py/_hl/dataset.py:793\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    791\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(selection\u001b[38;5;241m.\u001b[39mmshape)\n\u001b[1;32m    792\u001b[0m fspace \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Patch up the output for NumPy\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:192\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_proxy.pyx:141\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_conv.pyx:646\u001b[0m, in \u001b[0;36mh5py._conv.vlen2ndarray\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:434\u001b[0m, in \u001b[0;36mh5py.h5t.TypeID.dtype.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:435\u001b[0m, in \u001b[0;36mh5py.h5t.TypeID.dtype.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:955\u001b[0m, in \u001b[0;36mh5py.h5t.TypeIntegerID.py_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "epoch_desc = 'Epoch {{0: <{0}d}}'.format(1 + int(math.log10(args.num_epochs)))\n",
    "for epoch in range(args.num_epochs):\n",
    "    metalearner.train(meta_train_dataloader,\n",
    "                      max_batches=args.num_batches,\n",
    "                      verbose=args.verbose,\n",
    "                      desc='Training',\n",
    "                      leave=False)\n",
    "    results = metalearner.evaluate(meta_val_dataloader,\n",
    "                                    max_batches=args.num_batches,\n",
    "                                    verbose=args.verbose,\n",
    "                                    desc=epoch_desc.format(epoch + 1))\n",
    "\n",
    "    # Save best model\n",
    "    if 'accuracies_after' in results:\n",
    "        if (best_value is None) or (best_value < results['accuracies_after']):\n",
    "            best_value = results['accuracies_after']\n",
    "            save_model = True\n",
    "    elif (best_value is None) or (best_value > results['mean_outer_loss']):\n",
    "        best_value = results['mean_outer_loss']\n",
    "        save_model = True\n",
    "    else:\n",
    "        save_model = False\n",
    "\n",
    "    if save_model and (args.output_folder is not None):\n",
    "        with open(args.model_path, 'wb') as f:\n",
    "            torch.save(benchmark.model.state_dict(), f)\n",
    "\n",
    "if hasattr(benchmark.meta_train_dataset, 'close'):\n",
    "    benchmark.meta_train_dataset.close()\n",
    "    benchmark.meta_val_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   1%|     | 1/100 [01:32<2:32:55, 92.68s/it, accuracy=0.0696, loss=3.8067]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_test_dataloader = BatchMetaDataLoader(benchmark.meta_test_dataset,\n",
    "                                               batch_size=args.num_batches,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=args.num_workers,\n",
    "                                               pin_memory=True)\n",
    "\n",
    "results = metalearner.evaluate(meta_test_dataloader,\n",
    "                                   max_batches=args.num_batches,\n",
    "                                   verbose=args.verbose,\n",
    "                                   desc='Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
